{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd5e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyrealm/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n"
     ]
    }
   ],
   "source": [
    "from pyrealm.pmodel import (\n",
    "    SubdailyScaler,\n",
    "    memory_effect,\n",
    "    SubdailyPModel,\n",
    "    PModelEnvironment,\n",
    "    PModel,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyrealm.pmodel.optimal_chi import OptimalChiPrentice14\n",
    "from pyrealm.pmodel.functions import calc_ftemp_arrh, calc_ftemp_kphio,calc_modified_arrhenius_factor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyrealm.constants import PModelConst\n",
    "pmodel_const = PModelConst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777390c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyrealm.pmodel import SubdailyPModel, PModelEnvironment, SubdailyScaler\n",
    "\n",
    "# ==== replace with your input  i.e. fluxnet2015 + modis LAI ====\n",
    "fluxnet_root = \"FLX_CA-Oas_FLUXNET2015_SUBSET_HH_1996-2010_1-4.csv\"\n",
    "lai_file = \"CA-Oas_MCD15A3H_FPAR_LAI_QC.csv\"\n",
    "\n",
    "# read 4-day modis\n",
    "lai_data = pd.read_csv(lai_file)\n",
    "lai_data['date'] = pd.to_datetime(lai_data['datetime'], dayfirst=True, errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c347fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files\n",
      "Processing MY-PSO ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyrealm/2863226631.py:92: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "pyrealm/2863226631.py:93: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check input variables: Tair[19.294~34.77], patm[98925.0~100329.0], vpd[0.0~3362.7000000000003], ppfd[0.0~2365.0332], co2[400.0~400.0]\n",
      "\n",
      "===== Check LAI and fAPAR: MY-PSO =====\n",
      "LAI length: 122736, fAPAR length: 122736\n",
      "LAI range: 0.00 ~ 7.00\n",
      "fAPAR range: 0.00 ~ 0.97\n",
      "Missing ratio: LAI=0.00%, fAPAR=0.00%\n",
      "                 time  lai    fapar\n",
      "0 2003-01-01 00:00:00  0.8  0.32968\n",
      "1 2003-01-01 00:30:00  0.8  0.32968\n",
      "2 2003-01-01 01:00:00  0.8  0.32968\n",
      "3 2003-01-01 01:30:00  0.8  0.32968\n",
      "4 2003-01-01 02:00:00  0.8  0.32968\n",
      "5 2003-01-01 02:30:00  0.8  0.32968\n",
      "6 2003-01-01 03:00:00  0.8  0.32968\n",
      "7 2003-01-01 03:30:00  0.8  0.32968\n",
      "8 2003-01-01 04:00:00  0.8  0.32968\n",
      "9 2003-01-01 04:30:00  0.8  0.32968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyrealm/subdaily.py:261: ExperimentalFeatureWarning: 'This is a draft implementation and the API and calculations may change'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BR-Sa1 ...\n",
      "BR-Sa1: Detected hourly data, expanding to half-hourly...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_interval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1800\u001b[39m:  \u001b[38;5;66;03m# hourly data\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msite_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Detected hourly data, expanding to half-hourly...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mexpand_hourly_to_halfhourly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m site_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBR-Sa1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== BR-Sa1 interpolated Fluxnet DataFrame (first 20 rows) =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mexpand_hourly_to_halfhourly\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     13\u001b[0m     expanded_index\u001b[38;5;241m.\u001b[39mextend([ts, ts \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)])\n\u001b[1;32m     14\u001b[0m     expanded_rows\u001b[38;5;241m.\u001b[39mextend([row, row])\n\u001b[0;32m---> 15\u001b[0m expanded \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m expanded \u001b[38;5;241m=\u001b[39m expanded\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expanded\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:945\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m--> 945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contents, columns\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:1070\u001b[0m, in \u001b[0;36mconvert_object_array\u001b[0;34m(content, dtype, dtype_backend, coerce_float)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1070\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [convert(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:1070\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             arr \u001b[38;5;241m=\u001b[39m maybe_cast_to_datetime(arr, dtype)\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1070\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/internals/construction.py:1030\u001b[0m, in \u001b[0;36mconvert_object_array.<locals>.convert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(arr):\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1030\u001b[0m         arr \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m            \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtry_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_to_nullable_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;66;03m# Notes on cases that get here 2023-02-15\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# 1) we DO get here when arr is all Timestamps and dtype=None\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# 2) disabling this doesn't break the world, so this must be\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m         \u001b[38;5;66;03m#    getting caught at a higher level\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;66;03m# 3) passing convert_non_numeric to maybe_convert_objects get this right\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;66;03m# 4) convert_non_numeric?\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyrealm.pmodel import PModelEnvironment, SubdailyPModel, SubdailyScaler\n",
    "\n",
    "# ==== Helper functions ====\n",
    "def expand_hourly_to_halfhourly(df):\n",
    "    \"\"\"Expand hourly data into half-hourly data\"\"\"\n",
    "    expanded_index = []\n",
    "    expanded_rows = []\n",
    "    for ts, row in df.iterrows():\n",
    "        expanded_index.extend([ts, ts + pd.Timedelta(minutes=30)])\n",
    "        expanded_rows.extend([row, row])\n",
    "    expanded = pd.DataFrame(expanded_rows, index=expanded_index)\n",
    "    expanded = expanded.sort_index()\n",
    "    return expanded\n",
    "\n",
    "def get_lai_and_fapar(site_id, flux_times, date_col='date'):\n",
    "    \"\"\"\n",
    "    site_id: string, site name (case-insensitive)\n",
    "    flux_times: pandas.Series or DatetimeIndex (half-hourly time axis)\n",
    "    date_col: column name of date in the MODIS table, could be 'date' or 'datetime'\n",
    "    requires external lai_data: must contain columns ['SiteID', date_col, 'Lai'] (optional 'Fpar')\n",
    "    \"\"\"\n",
    "    # 1) Filter by site (normalize case and whitespace)\n",
    "    sub = lai_data[lai_data['SiteID'].str.strip().str.lower() == site_id.strip().lower()].copy()\n",
    "    if sub.empty:\n",
    "        # No data for this site: return all NaN (or apply default value strategy here)\n",
    "        n = len(flux_times)\n",
    "        return np.full(n, np.nan), np.full(n, np.nan)\n",
    "\n",
    "    # 2) Normalize date & numeric type\n",
    "    sub[date_col] = pd.to_datetime(sub[date_col], errors='coerce')\n",
    "    sub = sub.dropna(subset=[date_col])\n",
    "    sub['Lai'] = pd.to_numeric(sub.get('Lai'), errors='coerce')\n",
    "\n",
    "    # 3) Aggregate by day, remove duplicates within the same day\n",
    "    daily_lai = sub.groupby(sub[date_col].dt.normalize())['Lai'].mean()\n",
    "\n",
    "    # 4) Build complete daily time axis and interpolate (fill edges forward/backward)\n",
    "    start_day = pd.to_datetime(flux_times.min()).normalize()\n",
    "    end_day   = pd.to_datetime(flux_times.max()).normalize()\n",
    "    all_days = pd.date_range(start_day, end_day, freq='D')\n",
    "\n",
    "    daily_lai = (daily_lai\n",
    "                 .reindex(all_days)               \n",
    "                 .interpolate('linear')           \n",
    "                 .bfill()\n",
    "                 .ffill())\n",
    "\n",
    "    # 5) Map to half-hourly time axis (match by day)\n",
    "    flux_days = pd.to_datetime(pd.Series(flux_times)).dt.normalize()\n",
    "    lai_series = daily_lai.reindex(flux_days, method='nearest').to_numpy()\n",
    "\n",
    "    # 6) Calculate fAPAR\n",
    "    k = 0.5\n",
    "    fapar = 1 - np.exp(-k * lai_series)\n",
    "\n",
    "    return lai_series, fapar\n",
    "\n",
    "\n",
    "# ==== Main loop ====\n",
    "all_sites_daily = []\n",
    "all_sites_subdaily = []\n",
    "\n",
    "file_list = glob.glob(os.path.join(fluxnet_root, \"**/*H*.csv\"), recursive=True)\n",
    "print(f\"Found {len(file_list)} files\")\n",
    "\n",
    "for filepath in file_list:\n",
    "    site_id = filepath.split(\"/\")[-1].split(\"_\")[1]\n",
    "    print(f\"Processing {site_id} ...\")\n",
    "\n",
    "    # Read FLUXNET data\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['TIMESTAMP_START'] = df['TIMESTAMP_START'].astype(str).str.zfill(12)\n",
    "    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP_START'], format='%Y%m%d%H%M', errors='coerce')\n",
    "    df = df.set_index('TIMESTAMP')\n",
    "    df = df.replace(-9999, np.nan).dropna(subset=['TA_F','PA_F','VPD_F','SW_IN_F','GPP_NT_VUT_REF'])\n",
    "\n",
    "    # Detect resolution and expand\n",
    "    time_diffs = df.index.to_series().diff().dropna().dt.total_seconds()\n",
    "    avg_interval = time_diffs.mean()\n",
    "    if avg_interval > 1800:  # hourly data\n",
    "        print(f\"{site_id}: Detected hourly data, expanding to half-hourly...\")\n",
    "        df = expand_hourly_to_halfhourly(df)\n",
    "        if site_id == \"BR-Sa1\":\n",
    "            print(\"\\n===== BR-Sa1 interpolated Fluxnet DataFrame (first 20 rows) =====\")\n",
    "            print(df.head(20))\n",
    "\n",
    "    # Regenerate complete half-hourly time index, ensure regularity\n",
    "    full_time_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='30T')\n",
    "    df = df.reindex(full_time_index).interpolate().ffill()\n",
    "    times = df.index.to_numpy()\n",
    "\n",
    "    # Extract variables\n",
    "    Tair = np.clip(df['TA_F'].values, -25, None)  # constrain temperature\n",
    "    patm = df['PA_F'].values * 1000  # kPa -> Pa\n",
    "    vpd  = df['VPD_F'].values * 100  # kPa -> Pa\n",
    "    ppfd = df['SW_IN_F'].values * 2.04\n",
    "    co2  = df['CO2_F_MDS'].fillna(400).values  # default 400 ppm\n",
    "\n",
    "    # Extract variables and check\n",
    "    Tair = np.clip(df['TA_F'].values, -25, None)  # constrain temperature range, °C\n",
    "    patm = df['PA_F'].values * 1000  # kPa → Pa\n",
    "    vpd  = df['VPD_F'].values * 100  # hPa → Pa\n",
    "    ppfd = df['SW_IN_F'].values * 2.04  # SW → PPFD\n",
    "    co2  = df['CO2_F_MDS'].fillna(400).values  # fill missing with 400 ppm\n",
    "\n",
    "    # Check for NaN or abnormal values\n",
    "    print(f\"Check input variables: Tair[{np.nanmin(Tair)}~{np.nanmax(Tair)}], \"\n",
    "        f\"patm[{np.nanmin(patm)}~{np.nanmax(patm)}], \"\n",
    "        f\"vpd[{np.nanmin(vpd)}~{np.nanmax(vpd)}], \"\n",
    "        f\"ppfd[{np.nanmin(ppfd)}~{np.nanmax(ppfd)}], \"\n",
    "        f\"co2[{np.nanmin(co2)}~{np.nanmax(co2)}]\")\n",
    "\n",
    "    # Interpolate LAI & fAPAR\n",
    "    lai, fapar = get_lai_and_fapar(site_id, pd.Series(df.index))\n",
    "\n",
    "    # Output first 10 records of LAI and fAPAR\n",
    "    debug_df = pd.DataFrame({\n",
    "        'time': df.index[:10],\n",
    "        'lai': lai[:10],\n",
    "        'fapar': fapar[:10]\n",
    "    })\n",
    "    print(debug_df)\n",
    "\n",
    "    # SubdailyScaler\n",
    "    fs_scaler = SubdailyScaler(times)\n",
    "    fs_scaler.set_window(window_center=np.timedelta64(12, \"h\"), half_width=np.timedelta64(30, \"m\"))\n",
    "\n",
    "    # Model environment & run\n",
    "    env_subdaily = PModelEnvironment(tc=Tair, patm=patm, vpd=vpd, co2=co2)\n",
    "\n",
    "    subdailyC3 = SubdailyPModel(env=env_subdaily, method_kphio='temperature',\n",
    "                                 reference_kphio=0.125, method_optchi='prentice14',\n",
    "                                 fapar=fapar, ppfd=ppfd, fs_scaler=fs_scaler,\n",
    "                                 alpha=1/15, allow_holdover=True)\n",
    "\n",
    "    subdailyC4 = SubdailyPModel(env=env_subdaily, method_kphio='temperature',\n",
    "                                 reference_kphio=0.125, method_optchi='c4_no_gamma',\n",
    "                                 fapar=fapar, ppfd=ppfd, fs_scaler=fs_scaler,\n",
    "                                 alpha=1/15, allow_holdover=True)\n",
    "\n",
    "    # Subdaily output\n",
    "    subdaily_df = pd.DataFrame({\n",
    "        'time': times,\n",
    "        'GPP_c3': subdailyC3.gpp / 12,\n",
    "        'GPP_c4': subdailyC4.gpp / 12,\n",
    "        'vcmax_opt': subdailyC3.subdaily_vcmax25,\n",
    "        'jmax_opt': subdailyC3.subdaily_jmax25,\n",
    "        'temperature': Tair,\n",
    "        'lai': lai,\n",
    "        'fapar': fapar,\n",
    "        'FLUXNET_GPP': df['GPP_NT_VUT_REF'].values,\n",
    "        'PPFD':ppfd\n",
    "    })\n",
    "    subdaily_df['site'] = site_id\n",
    "    subdaily_df['date'] = pd.to_datetime(subdaily_df['time']).dt.date\n",
    "\n",
    "    # Daily output\n",
    "    daily_acclimation_temps = fs_scaler.get_daily_means(Tair)\n",
    "    daily_env  = {\n",
    "        'time':pd.to_datetime(times).floor(\"D\").unique(),\n",
    "        'temp_acclim': fs_scaler.get_daily_means(Tair),\n",
    "        'co2_acclim': fs_scaler.get_daily_means(co2),\n",
    "        'vpd_acclim': fs_scaler.get_daily_means(vpd),\n",
    "        'patm_acclim': fs_scaler.get_daily_means(patm),\n",
    "        'ppfd_acclim': fs_scaler.get_daily_means(ppfd),\n",
    "        'fapar_acclim': fs_scaler.get_daily_means(fapar),\n",
    "    }\n",
    "\n",
    "    # 2. Add this series as new columns into half-hourly DataFrame\n",
    "    daily_env_df = pd.DataFrame(daily_env)\n",
    "    daily_env_df['date'] = pd.to_datetime(daily_env['time']).date\n",
    "    daily_env_df['site'] = site_id\n",
    "    daily_df = subdaily_df.groupby('date').mean(numeric_only=True).reset_index()\n",
    "    daily_df['site'] = site_id\n",
    "    daily_df = pd.merge(daily_env_df,daily_df,on = ['site','date'])\n",
    "    all_sites_subdaily.append(subdaily_df)\n",
    "    all_sites_daily.append(daily_df)\n",
    "\n",
    "# Merge and save\n",
    "df_subdaily = pd.concat(all_sites_subdaily)\n",
    "df_daily = pd.concat(all_sites_daily)\n",
    "#save to local directory\n",
    "#df_daily.to_csv(\"all_sites_daily_LAI_FPAR.csv\", index=False)\n",
    "#df_subdaily.to_csv(\"all_sites_subdaily_LAI_FPAR.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

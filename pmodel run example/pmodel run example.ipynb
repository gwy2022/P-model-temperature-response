{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd5e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyrealm/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n"
     ]
    }
   ],
   "source": [
    "from pyrealm.pmodel import (\n",
    "    SubdailyScaler,\n",
    "    memory_effect,\n",
    "    SubdailyPModel,\n",
    "    PModelEnvironment,\n",
    "    PModel,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyrealm.pmodel.optimal_chi import OptimalChiPrentice14\n",
    "from pyrealm.pmodel.functions import calc_ftemp_arrh, calc_ftemp_kphio,calc_modified_arrhenius_factor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyrealm.constants import PModelConst\n",
    "pmodel_const = PModelConst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777390c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyrealm.pmodel import SubdailyPModel, PModelEnvironment, SubdailyScaler\n",
    "\n",
    "# ==== replace with your input  i.e. fluxnet2015 + modis LAI ====\n",
    "fluxnet_root = \"FLX_CA-Oas_FLUXNET2015_SUBSET_HH_1996-2010_1-4.csv\"\n",
    "lai_file = \"CA-Oas_MCD15A3H_FPAR_LAI_QC.csv\"\n",
    "\n",
    "# read 4-day modis\n",
    "lai_data = pd.read_csv(lai_file)\n",
    "lai_data['date'] = pd.to_datetime(lai_data['datetime'], dayfirst=True, errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c347fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyrealm.pmodel import PModelEnvironment, SubdailyPModel, SubdailyScaler\n",
    "\n",
    "# ==== Helper functions ====\n",
    "def expand_hourly_to_halfhourly(df):\n",
    "    \"\"\"Expand hourly data into half-hourly data\"\"\"\n",
    "    expanded_index = []\n",
    "    expanded_rows = []\n",
    "    for ts, row in df.iterrows():\n",
    "        expanded_index.extend([ts, ts + pd.Timedelta(minutes=30)])\n",
    "        expanded_rows.extend([row, row])\n",
    "    expanded = pd.DataFrame(expanded_rows, index=expanded_index)\n",
    "    expanded = expanded.sort_index()\n",
    "    return expanded\n",
    "\n",
    "def get_lai_and_fapar(site_id, flux_times, date_col='date'):\n",
    "    \"\"\"\n",
    "    site_id: string, site name (case-insensitive)\n",
    "    flux_times: pandas.Series or DatetimeIndex (half-hourly time axis)\n",
    "    date_col: column name of date in the MODIS table, could be 'date' or 'datetime'\n",
    "    requires external lai_data: must contain columns ['SiteID', date_col, 'Lai'] (optional 'Fpar')\n",
    "    \"\"\"\n",
    "    # 1) Filter by site (normalize case and whitespace)\n",
    "    sub = lai_data[lai_data['SiteID'].str.strip().str.lower() == site_id.strip().lower()].copy()\n",
    "    if sub.empty:\n",
    "        # No data for this site: return all NaN (or apply default value strategy here)\n",
    "        n = len(flux_times)\n",
    "        return np.full(n, np.nan), np.full(n, np.nan)\n",
    "\n",
    "    # 2) Normalize date & numeric type\n",
    "    sub[date_col] = pd.to_datetime(sub[date_col], errors='coerce')\n",
    "    sub = sub.dropna(subset=[date_col])\n",
    "    sub['Lai'] = pd.to_numeric(sub.get('Lai'), errors='coerce')\n",
    "\n",
    "    # 3) Aggregate by day, remove duplicates within the same day\n",
    "    daily_lai = sub.groupby(sub[date_col].dt.normalize())['Lai'].mean()\n",
    "\n",
    "    # 4) Build complete daily time axis and interpolate (fill edges forward/backward)\n",
    "    start_day = pd.to_datetime(flux_times.min()).normalize()\n",
    "    end_day   = pd.to_datetime(flux_times.max()).normalize()\n",
    "    all_days = pd.date_range(start_day, end_day, freq='D')\n",
    "\n",
    "    daily_lai = (daily_lai\n",
    "                 .reindex(all_days)               \n",
    "                 .interpolate('linear')           \n",
    "                 .bfill()\n",
    "                 .ffill())\n",
    "\n",
    "    # 5) Map to half-hourly time axis (match by day)\n",
    "    flux_days = pd.to_datetime(pd.Series(flux_times)).dt.normalize()\n",
    "    lai_series = daily_lai.reindex(flux_days, method='nearest').to_numpy()\n",
    "\n",
    "    # 6) Calculate fAPAR\n",
    "    k = 0.5\n",
    "    fapar = 1 - np.exp(-k * lai_series)\n",
    "\n",
    "    return lai_series, fapar\n",
    "\n",
    "\n",
    "# ==== Main loop ====\n",
    "all_sites_daily = []\n",
    "all_sites_subdaily = []\n",
    "\n",
    "file_list = glob.glob(os.path.join(fluxnet_root, \"**/*H*.csv\"), recursive=True)\n",
    "print(f\"Found {len(file_list)} files\")\n",
    "\n",
    "for filepath in file_list:\n",
    "    site_id = filepath.split(\"/\")[-1].split(\"_\")[1]\n",
    "    print(f\"Processing {site_id} ...\")\n",
    "\n",
    "    # Read FLUXNET data\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['TIMESTAMP_START'] = df['TIMESTAMP_START'].astype(str).str.zfill(12)\n",
    "    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP_START'], format='%Y%m%d%H%M', errors='coerce')\n",
    "    df = df.set_index('TIMESTAMP')\n",
    "    df = df.replace(-9999, np.nan).dropna(subset=['TA_F','PA_F','VPD_F','SW_IN_F','GPP_NT_VUT_REF'])\n",
    "\n",
    "    # Detect resolution and expand\n",
    "    time_diffs = df.index.to_series().diff().dropna().dt.total_seconds()\n",
    "    avg_interval = time_diffs.mean()\n",
    "    if avg_interval > 1800:  # hourly data\n",
    "        print(f\"{site_id}: Detected hourly data, expanding to half-hourly...\")\n",
    "        df = expand_hourly_to_halfhourly(df)\n",
    "        if site_id == \"BR-Sa1\":\n",
    "            print(\"\\n===== BR-Sa1 interpolated Fluxnet DataFrame (first 20 rows) =====\")\n",
    "            print(df.head(20))\n",
    "\n",
    "    # Regenerate complete half-hourly time index, ensure regularity\n",
    "    full_time_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='30T')\n",
    "    df = df.reindex(full_time_index).interpolate().ffill()\n",
    "    times = df.index.to_numpy()\n",
    "\n",
    "    # Extract variables\n",
    "    Tair = np.clip(df['TA_F'].values, -25, None)  # constrain temperature\n",
    "    patm = df['PA_F'].values * 1000  # kPa -> Pa\n",
    "    vpd  = df['VPD_F'].values * 100  # kPa -> Pa\n",
    "    ppfd = df['SW_IN_F'].values * 2.04\n",
    "    co2  = df['CO2_F_MDS'].fillna(400).values  # default 400 ppm\n",
    "\n",
    "    # Extract variables and check\n",
    "    Tair = np.clip(df['TA_F'].values, -25, None)  # constrain temperature range, °C\n",
    "    patm = df['PA_F'].values * 1000  # kPa → Pa\n",
    "    vpd  = df['VPD_F'].values * 100  # hPa → Pa\n",
    "    ppfd = df['SW_IN_F'].values * 2.04  # SW → PPFD\n",
    "    co2  = df['CO2_F_MDS'].fillna(400).values  # fill missing with 400 ppm\n",
    "\n",
    "    # Check for NaN or abnormal values\n",
    "    print(f\"Check input variables: Tair[{np.nanmin(Tair)}~{np.nanmax(Tair)}], \"\n",
    "        f\"patm[{np.nanmin(patm)}~{np.nanmax(patm)}], \"\n",
    "        f\"vpd[{np.nanmin(vpd)}~{np.nanmax(vpd)}], \"\n",
    "        f\"ppfd[{np.nanmin(ppfd)}~{np.nanmax(ppfd)}], \"\n",
    "        f\"co2[{np.nanmin(co2)}~{np.nanmax(co2)}]\")\n",
    "\n",
    "    # Interpolate LAI & fAPAR\n",
    "    lai, fapar = get_lai_and_fapar(site_id, pd.Series(df.index))\n",
    "\n",
    "    # Output first 10 records of LAI and fAPAR\n",
    "    debug_df = pd.DataFrame({\n",
    "        'time': df.index[:10],\n",
    "        'lai': lai[:10],\n",
    "        'fapar': fapar[:10]\n",
    "    })\n",
    "    print(debug_df)\n",
    "\n",
    "    # SubdailyScaler\n",
    "    fs_scaler = SubdailyScaler(times)\n",
    "    fs_scaler.set_window(window_center=np.timedelta64(12, \"h\"), half_width=np.timedelta64(30, \"m\"))\n",
    "\n",
    "    # Model environment & run\n",
    "    env_subdaily = PModelEnvironment(tc=Tair, patm=patm, vpd=vpd, co2=co2)\n",
    "\n",
    "    subdailyC3 = SubdailyPModel(env=env_subdaily, method_kphio='temperature',\n",
    "                                 reference_kphio=0.125, method_optchi='prentice14',\n",
    "                                 fapar=fapar, ppfd=ppfd, fs_scaler=fs_scaler,\n",
    "                                 alpha=1/15, allow_holdover=True)\n",
    "\n",
    "    subdailyC4 = SubdailyPModel(env=env_subdaily, method_kphio='temperature',\n",
    "                                 reference_kphio=0.125, method_optchi='c4_no_gamma',\n",
    "                                 fapar=fapar, ppfd=ppfd, fs_scaler=fs_scaler,\n",
    "                                 alpha=1/15, allow_holdover=True)\n",
    "\n",
    "    # Subdaily output\n",
    "    subdaily_df = pd.DataFrame({\n",
    "        'time': times,\n",
    "        'GPP_c3': subdailyC3.gpp / 12,\n",
    "        'GPP_c4': subdailyC4.gpp / 12,\n",
    "        'vcmax_opt': subdailyC3.subdaily_vcmax25,\n",
    "        'jmax_opt': subdailyC3.subdaily_jmax25,\n",
    "        'temperature': Tair,\n",
    "        'lai': lai,\n",
    "        'fapar': fapar,\n",
    "        'FLUXNET_GPP': df['GPP_NT_VUT_REF'].values,\n",
    "        'PPFD':ppfd\n",
    "    })\n",
    "    subdaily_df['site'] = site_id\n",
    "    subdaily_df['date'] = pd.to_datetime(subdaily_df['time']).dt.date\n",
    "\n",
    "    # Daily output\n",
    "    daily_acclimation_temps = fs_scaler.get_daily_means(Tair)\n",
    "    daily_env  = {\n",
    "        'time':pd.to_datetime(times).floor(\"D\").unique(),\n",
    "        'temp_acclim': fs_scaler.get_daily_means(Tair),\n",
    "        'co2_acclim': fs_scaler.get_daily_means(co2),\n",
    "        'vpd_acclim': fs_scaler.get_daily_means(vpd),\n",
    "        'patm_acclim': fs_scaler.get_daily_means(patm),\n",
    "        'ppfd_acclim': fs_scaler.get_daily_means(ppfd),\n",
    "        'fapar_acclim': fs_scaler.get_daily_means(fapar),\n",
    "    }\n",
    "\n",
    "    # 2. Add this series as new columns into half-hourly DataFrame\n",
    "    daily_env_df = pd.DataFrame(daily_env)\n",
    "    daily_env_df['date'] = pd.to_datetime(daily_env['time']).date\n",
    "    daily_env_df['site'] = site_id\n",
    "    daily_df = subdaily_df.groupby('date').mean(numeric_only=True).reset_index()\n",
    "    daily_df['site'] = site_id\n",
    "    daily_df = pd.merge(daily_env_df,daily_df,on = ['site','date'])\n",
    "    all_sites_subdaily.append(subdaily_df)\n",
    "    all_sites_daily.append(daily_df)\n",
    "\n",
    "# Merge and save\n",
    "df_subdaily = pd.concat(all_sites_subdaily)\n",
    "df_daily = pd.concat(all_sites_daily)\n",
    "#save to local directory\n",
    "#df_daily.to_csv(\"all_sites_daily_LAI_FPAR.csv\", index=False)\n",
    "#df_subdaily.to_csv(\"all_sites_subdaily_LAI_FPAR.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
